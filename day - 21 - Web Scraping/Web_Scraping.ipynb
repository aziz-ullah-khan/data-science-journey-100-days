{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cad9aeb",
   "metadata": {},
   "source": [
    "# üìò Web Scraping\n",
    "\n",
    "‚úçÔ∏è Aziz Ullah Khan | üìÖ July 16, 2024\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Overview\n",
    "\n",
    "Welcome to Day 21 of our journey! Today, we'll be diving into the world of web scraping. This notebook will guide you through the basics to advanced techniques of web scraping, using popular Python libraries.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction to Web Scraping](#Introduction-to-Web-Scraping)\n",
    "2. [Tools and Libraries](#Tools-and-Libraries)\n",
    "3. [Practical Examples](#Practical-Examples)\n",
    "4. [Advanced Techniques](#Advanced-Techniques)\n",
    "5. [Best Practices and Legal Considerations](#Best-Practices-and-Legal-Considerations)\n",
    "\n",
    "\n",
    "Let's deep dive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386337f",
   "metadata": {},
   "source": [
    "## Introduction to Web Scraping\n",
    "\n",
    "### What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of extracting data from websites. It involves fetching the web page, parsing the HTML or XML content, and extracting useful information.\n",
    "\n",
    "### Use Cases and Applications\n",
    "\n",
    "- Data analysis\n",
    "- Price monitoring\n",
    "- News aggregation\n",
    "- Market research\n",
    "- Social media sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76cd15",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "740b441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 selenium scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74025f5",
   "metadata": {},
   "source": [
    "## Tools and Libraries\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a Python library for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c825eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The News International: Latest News Breaking, Pakistan News\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://www.thenews.com.pk/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24eec23",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "\n",
    "Scrapy is an open-source web-crawling framework for Python. It's used to extract data from websites and process them as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aec562",
   "metadata": {},
   "source": [
    "### Selenium\n",
    "\n",
    "Selenium is a powerful tool for controlling a web browser through programs and performing browser automation. It's useful for scraping websites with dynamic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c43417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The News International: Latest News Breaking, Pakistan News\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.thenews.com.pk/')\n",
    "print(driver.title)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb0fee",
   "metadata": {},
   "source": [
    "## Practical Examples\n",
    "\n",
    "### Extracting data from HTML\n",
    "\n",
    "In this section, we'll demonstrate how to extract specific data from a web page using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae66dd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.thenews.com.pk/\n",
      "https://www.thenews.com.pk/latest-stories\n",
      "https://www.thenews.com.pk/latest/category/national\n",
      "https://www.thenews.com.pk/latest/category/sports\n",
      "https://www.thenews.com.pk/latest/category/world\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.thenews.com.pk/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "for link in soup.find_all('a')[:5]: # display only 5\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb28589",
   "metadata": {},
   "source": [
    "### Navigating and parsing websites\n",
    "\n",
    "BeautifulSoup allows you to navigate the parse tree and extract data from nested elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c8873c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest News\n",
      "National\n",
      "Sports\n",
      "World\n",
      "Business\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('li')[:5]: # display only 5\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecea97a",
   "metadata": {},
   "source": [
    "#### Find the element by Class Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c33e4e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIVE\n",
      "Displacement, shortages of food, hospitals endanger pregnant women in Gaza: UN\n",
      "Water scarcity, \n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the specified URL\n",
    "driver.get('https://www.thenews.com.pk/')\n",
    "\n",
    "# Find the element by ID\n",
    "content = driver.find_element(By.CLASS_NAME, 'siteContent')\n",
    "\n",
    "# Print the text of the element\n",
    "print(content.text[:100]) # display only 100 characters\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be51550",
   "metadata": {},
   "source": [
    "## Advanced Techniques\n",
    "\n",
    "### Bypassing anti-scraping mechanisms\n",
    "\n",
    "\n",
    "Websites often have measures to detect and block web scraping. To bypass these mechanisms, you can:\n",
    "\n",
    "    - Rotate IP addresses\n",
    "    - Use proxies\n",
    "    - Set appropriate headers\n",
    "\n",
    "Example code for setting headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6886e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://example.com'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d12b4",
   "metadata": {},
   "source": [
    "## Managing Sessions and Cookies\n",
    "\n",
    "Handling sessions and cookies is crucial for scraping websites that require login.\n",
    "\n",
    "Example code for managing sessions and cookies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8110f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "login_url = 'http://example.com/login'\n",
    "url = 'http://example.com/some-page' \n",
    "payload = {'username': 'user', 'password': 'pass'}\n",
    "\n",
    "session = requests.Session()\n",
    "session.post(login_url, data=payload)\n",
    "response = session.get(url)\n",
    "\n",
    "print(response.text[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732054dd",
   "metadata": {},
   "source": [
    "## Scraping Large Websites Efficiently\n",
    "\n",
    "Using Scrapy, you can build a spider to crawl and scrape large websites efficiently.\n",
    "\n",
    "Example code for a simple Scrapy spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80db5f51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'‚ÄúThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.‚Äù' - Albert Einstein\n",
      "'‚ÄúIt is our choices, Harry, that show what we truly are, far more than our abilities.‚Äù' - J.K. Rowling\n",
      "'‚ÄúThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.‚Äù' - Albert Einstein\n",
      "'‚ÄúThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.‚Äù' - Jane Austen\n",
      "'‚ÄúImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.‚Äù' - Marilyn Monroe\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "# Define a Scrapy Spider\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Run the Scrapy Spider\n",
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"quotes.json\": {\"format\": \"json\"},\n",
    "    },\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()\n",
    "\n",
    "# Read the Scraped Data\n",
    "with open('quotes.json') as f:\n",
    "    quotes = json.load(f)\n",
    "\n",
    "for quote in quotes[:5]:\n",
    "    print(f\"'{quote['text']}' - {quote['author']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6093725",
   "metadata": {},
   "source": [
    "## Best Practices and Legal Considerations\n",
    "\n",
    "### Ethical scraping\n",
    "\n",
    "Always follow ethical guidelines when scraping websites. Make sure to:\n",
    "\n",
    "- Respect the website's `robots.txt` file\n",
    "- Avoid overloading the server with too many requests\n",
    "- Use appropriate headers to simulate a real user\n",
    "\n",
    "### Respecting robots.txt\n",
    "\n",
    "`robots.txt` is a file that webmasters use to give instructions about their site to web robots. Always check and respect the directives in this file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f0a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('http://example.com/robots.txt')\n",
    "rp.read()\n",
    "print(rp.can_fetch('*', 'http://example.com/some-page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bafcb8d",
   "metadata": {},
   "source": [
    "üåê Feel free to connect with [me](https://www.linkedin.com/in/aziz-ullah-khan/) if you have questions or want to discuss this fascinating journey further! Let's continue exploring together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
